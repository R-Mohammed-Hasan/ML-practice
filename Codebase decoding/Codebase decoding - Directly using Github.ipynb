{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10577320,"sourceType":"datasetVersion","datasetId":6545693}],"dockerImageVersionId":30839,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport re\n\ndef load_codebase(directory_path):\n    \"\"\"\n    Loads all files from a directory and returns a dictionary with filenames as keys.\n    \"\"\"\n    code_files = {}\n    for root, _, files in os.walk(directory_path):\n        for file in files:\n            if file.endswith(('.rb', '.js', '.erb', '.html', '.css')) or any(s in file for s in ['Gemfile', 'package.json']):\n                file_path = os.path.join(root, file)\n                print(file_path)\n                with open(file_path, 'r', encoding='utf-8') as f:\n                    code_files[file_path] = f.read()\n    return code_files\n\ndef preprocess_code(file_content):\n    \"\"\"''\n    Splits code into logical chunks (e.g., functions or classes) with their context.\n    \"\"\"\n    chunks = re.split(r'\\n\\s*\\n', file_content)  # Split by blank lines\n    return [chunk.strip() for chunk in chunks if chunk.strip()]\n\n# Example usage\ncodebase = load_codebase('/kaggle/input/webspire-github-repository')  # Provide the codebase directory path\npreprocessed_code = {file: preprocess_code(content) for file, content in codebase.items()}\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sentence_transformers import SentenceTransformer\nimport numpy as np\n\n# Load a pre-trained embedding model\nembedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n\n# Create embeddings for each chunk\ndef create_embeddings(preprocessed_code):\n    \"\"\"\n    Converts preprocessed code chunks into embeddings for semantic search.\n    \"\"\"\n    embeddings = {}\n    for file, chunks in preprocessed_code.items():\n        if chunks:\n            chunk_embeddings = embedding_model.encode(chunks, convert_to_tensor=False)  # Generate embeddings\n            chunk_embeddings = np.array(chunk_embeddings)  # Ensure it's a NumPy array\n            if chunk_embeddings.ndim == 1:  # If 1D, reshape to 2D\n                chunk_embeddings = chunk_embeddings.reshape(1, -1)\n            embeddings[file] = {\n                \"chunks\": chunks,\n                \"embeddings\": chunk_embeddings,\n            }\n    return embeddings\n\nembeddings = create_embeddings(preprocessed_code)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Debug: Check the structure of the `embeddings` dictionary\nprint(f\"Number of files processed: {len(embeddings)}\")\nfor file, data in embeddings.items():\n    print(f\"File: {file}, Number of chunks: {len(data['chunks'])}, Embedding Shape: {data['embeddings'].shape}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import pipeline\n\n# Initialize the LLM pipeline\nllm = pipeline(\n    \"text-generation\",\n    model=\"tiiuae/falcon-7b-instruct\",\n    trust_remote_code=True,\n    device_map=\"auto\",\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics.pairwise import cosine_similarity\n\ndef retrieve_relevant_chunks(query, embeddings, top_k=3):\n    \"\"\"\n    Retrieves the top-k relevant code chunks for a given query.\n    \"\"\"\n    # Ensure query_embedding is reshaped as a 2D array\n    query_embedding = embedding_model.encode(query, convert_to_tensor=False).reshape(1, -1)\n\n    results = []\n\n    for file, data in embeddings.items():\n        # Ensure chunk embeddings are 2D\n        chunk_embeddings = np.array(data[\"embeddings\"])\n        if chunk_embeddings.ndim == 1:\n            chunk_embeddings = chunk_embeddings.reshape(1, -1)\n\n        # Compute cosine similarity\n        similarities = cosine_similarity(query_embedding, chunk_embeddings).flatten()\n\n        # Get top-k indices for this file\n        top_indices = np.argsort(similarities)[-top_k:][::-1]\n        for idx in top_indices:\n            results.append((file, data[\"chunks\"][idx], similarities[idx]))\n\n    # Sort overall results by similarity\n    results = sorted(results, key=lambda x: x[2], reverse=True)\n    return results[:top_k]\n\n\n# Example usage\nquery = \"How the images & videos uploaded to a post are stored ?\"\nretrieved_chunks = retrieve_relevant_chunks(query, embeddings)\nfor file, chunk, similarity in retrieved_chunks:\n    print(f\"File: {file}\\nSimilarity: {similarity}\\nChunk:\\n{chunk}\\n\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-25T12:22:18.473906Z","iopub.execute_input":"2025-01-25T12:22:18.474742Z","iopub.status.idle":"2025-01-25T12:22:19.150343Z","shell.execute_reply.started":"2025-01-25T12:22:18.474665Z","shell.execute_reply":"2025-01-25T12:22:19.148282Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b933e15f91c449a8a6c1ee655c2e12ae"}},"metadata":{}},{"name":"stdout","text":"File: /kaggle/input/webspire-github-repository/config/environments/production.rb\nSimilarity: 0.408914715051651\nChunk:\n# Store uploaded files on the local file system (see config/storage.yml for options).\n  config.active_storage.service = :local\n\nFile: /kaggle/input/webspire-github-repository/config/environments/development.rb\nSimilarity: 0.4089146852493286\nChunk:\n# Store uploaded files on the local file system (see config/storage.yml for options).\n  config.active_storage.service = :local\n\nFile: /kaggle/input/webspire-github-repository/app/models/post.rb\nSimilarity: 0.3798864483833313\nChunk:\nvalidates :posts, presence: true, blob: { content_type: ['image/png', 'image/jpg', 'image/jpeg', 'video/mp4'] }\n\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"\ndef generate_answer(query, retrieved_chunks):\n    \"\"\"\n    Uses the retrieved chunks as context to generate an answer.\n    \"\"\"\n    context = \"\\n\\n\".join([chunk for _, chunk, _ in retrieved_chunks])\n    prompt = (\n        f\"Context:\\n{context}\\n\\n\"\n        f\"Question: {query}\\n\"\n        f\"Answer:\"\n    )\n    response = llm(\n        prompt,\n        max_new_tokens=100,\n        num_return_sequences=1,\n        do_sample=False,\n    )\n    return response[0]['generated_text']\n\n# Example usage\nanswer = generate_answer(query, retrieved_chunks)\nprint(\"Generated Answer:\\n\", answer)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-25T12:22:25.785350Z","iopub.execute_input":"2025-01-25T12:22:25.785858Z"}},"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n","output_type":"stream"}],"execution_count":null}]}