{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10577320,"sourceType":"datasetVersion","datasetId":6545693}],"dockerImageVersionId":30839,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport re\n\ndef load_codebase(directory_path):\n    \"\"\"\n    Loads all files from a directory and returns a dictionary with filenames as keys.\n    \"\"\"\n    code_files = {}\n    for root, _, files in os.walk(directory_path):\n        for file in files:\n            if file.endswith(('.rb', '.js', '.erb', '.html', '.css')):\n                file_path = os.path.join(root, file)\n                with open(file_path, 'r', encoding='utf-8') as f:\n                    code_files[file_path] = f.read()\n    return code_files\n\ndef preprocess_code(file_content):\n    \"\"\"\n    Splits code into logical chunks (e.g., functions or classes) with their context.\n    \"\"\"\n    chunks = re.split(r'\\n\\s*\\n', file_content)  # Split by blank lines\n    return [chunk.strip() for chunk in chunks if chunk.strip()]\n\n# Example usage\ncodebase = load_codebase('/kaggle/input/webspire-github-repository')  # Provide the codebase directory path\npreprocessed_code = {file: preprocess_code(content) for file, content in codebase.items()}\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-25T11:32:54.403760Z","iopub.execute_input":"2025-01-25T11:32:54.404098Z","iopub.status.idle":"2025-01-25T11:32:55.350690Z","shell.execute_reply.started":"2025-01-25T11:32:54.404074Z","shell.execute_reply":"2025-01-25T11:32:55.349473Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"from sentence_transformers import SentenceTransformer\nimport numpy as np\n\n# Load a pre-trained embedding model\nembedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n\n# Create embeddings for each chunk\ndef create_embeddings(preprocessed_code):\n    embeddings = {}\n    for file, chunks in preprocessed_code.items():\n        embeddings[file] = {\n            \"chunks\": chunks,\n            \"embeddings\": embedding_model.encode(chunks, convert_to_tensor=True)\n        }\n    return embeddings\n\nembeddings = create_embeddings(preprocessed_code)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics.pairwise import cosine_similarity\n\ndef retrieve_relevant_chunks(query, embeddings, top_k=3):\n    \"\"\"\n    Retrieves the top-k relevant code chunks for a given query.\n    \"\"\"\n    query_embedding = embedding_model.encode(query, convert_to_tensor=True)\n    results = []\n\n    for file, data in embeddings.items():\n        similarities = cosine_similarity(query_embedding.reshape(1, -1), data[\"embeddings\"]).flatten()\n        top_indices = np.argsort(similarities)[-top_k:][::-1]\n        for idx in top_indices:\n            results.append((file, data[\"chunks\"][idx], similarities[idx]))\n    \n    # Sort overall results by similarity\n    results = sorted(results, key=lambda x: x[2], reverse=True)\n    return results[:top_k]\n\n# Example usage\nquery = \"Which library is used for infinite scrolling?\"\nretrieved_chunks = retrieve_relevant_chunks(query, embeddings)\nfor file, chunk, similarity in retrieved_chunks:\n    print(f\"File: {file}\\nSimilarity: {similarity}\\nChunk:\\n{chunk}\\n\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import pipeline\n\n# Initialize the LLM pipeline\nllm = pipeline(\n    \"text-generation\",\n    model=\"tiiuae/falcon-7b-instruct\",\n    trust_remote_code=True,\n    device_map=\"auto\",\n)\n\ndef generate_answer(query, retrieved_chunks):\n    \"\"\"\n    Uses the retrieved chunks as context to generate an answer.\n    \"\"\"\n    context = \"\\n\\n\".join([chunk for _, chunk, _ in retrieved_chunks])\n    prompt = (\n        f\"Context:\\n{context}\\n\\n\"\n        f\"Question: {query}\\n\"\n        f\"Answer:\"\n    )\n    response = llm(\n        prompt,\n        max_new_tokens=100,\n        num_return_sequences=1,\n        do_sample=False,\n    )\n    return response[0]['generated_text']\n\n# Example usage\nanswer = generate_answer(query, retrieved_chunks)\nprint(\"Generated Answer:\\n\", answer)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}